
\chapter{Results}
\lhead{\emph{Results}}
In this chapter, the results of each classification method are compared. Then, advantages and disadvantage of these methods are discussed.

\section{Comparison of Accuracy Scores}

Results of the best classification model of each method tried are shown in  \autoref{table:all}. From this table, it can be seen that transfer learning is the best classification method with 98.20 percent accuracy. The convolutional neural network is in the second place with 69.34 percent accuracy. It has to be considered that with transfer learning a classification model was not built from the ground up. The difference in the accuracy score between other classification methods is not that marginal. The lowest accuracy  - 51.53 percent was achieved by the Linear classifier while the support vector machine model achieved 60.73 percent accuracy.  From these results, it can be concluded that the convolutional neural network achieved a much higher accuracy score than other classifiers that were built from the ground up.

\begin{table}[h]
\begin{center}
\begin{tabular}{ |c|c| } 
 \hline
Method &  Classification Accuracy \\   \hline
KNN    &  56.32\% \\
SVM with PCA   & 60.73\% \\
Linear   &    51.53\%  \\
3 Layer    &   54.02\% \\
5 Layer    &  58.81\% \\
CNN & 69.34\% \\
Transfer  &  98.20\% \\

 \hline
\end{tabular}
\caption{Best Classifiers of Each Method}
\label{table:all}
\end{center}
\end{table}


\section{Comparison  of Time Complexity of the Classifiers}
There are three kinds of time complexities that can be considered for evaluating machine learning classifiers. The first is a build-time complexity - how long does it take to create a classification model. The second is a training complexity - the amount that it takes to train a classification model. The final is a runtime complexity - how long does the model take to classify an item. 

The K-nearest neighbours classifier was simple to build. It only required a hyper-parameter optimisation which did not take long. The training of the algorithm was also fast. In order to train this algorithm, the dataset was simply saved to the memory. However, the runtime performance of this classifier was not that great. During the runtime, this algorithm has to calculate distances between an unseen data sample and every other element in the dataset. Later, these distances are compared to find the K closest distances. Because these are computationally expensive tasks, it takes a long time for K-NN classifier to make a prediction.

It was a much more difficult task to get Support Vector Machine Algorithm to work. Despite the fact that the algorithm is designed to work on a high dimensional data,  the food image dataset was too complex for it. The only way to use this algorithm was by firstly applying the PCA to the dataset. Therefore, building this model required a significant amount of experimentation and took a much longer time, before actual results were seen. It took a very long time to train the support vector machine. Prediction time was faster than the prediction time of the K-NN classifier but was not instantaneous.

The linear classifier is a very simple classification model. Therefore, it did not take much time to create this model. Training of this algorithm was fast because the only things calculated were a matrix product of training images and weights, and updated weights, computed by an optimisation algorithm. Prediction time was almost immediate.

The Deep neural network required much more experimentation, to make it work. Training of the algorithm took a long time, but the prediction was almost as fast as the linear classification model.

The convolutional neural network required even more engineering work. Some CNN models architectures that were tried had a very low classification accuracy. Various models had to be tested, in order to finally find the architecture that was working.  Training of the algorithm took the longest amount of time from all of the algorithms implemented. However, the classifier was able to predict the classes of new pictures very quickly.

The transfer learning technique was simple because the model with a python script was provided by Google. In a case where one has to write a script that retrains the final layer,  it can be a very hard task. The calculation of the bottlenecks took a long time. Therefore, the training complexity of this algorithm was high.  However, for classifying the images, this algorithm took the same time as other neural networks.

To conclude, the algorithm that took the least time to built was KNN. Deep neural network models took the longest amount of time to build because it was a very hard task to find an appropriate network architecture. The classification algorithm that took the least time to train was KNN. For fastest prediction times, the linear classifier was the best option.

\section{Possible Improvements and Limitations of Classification Models}

The KNN classifier, the SVM,  and the linear classifier model cannot be considerably improved. The Hyperparameters of these algorithms were optimised by using a grid or random search optimisation methods. Therefore, the only possibility for improving these models is training on a more powerful machine and using  larger sizes of images.

In contrast,  the accuracy of the deep neural network and the convolutional neural network could still be considerably improved. It can be done by adding more layers or modifying the structure of the networks. However, it cannot be mathematically calculate which structures of neural networks work the best. The number of ways that these networks can be configured is infinite. Therefore, finding optimal structures of neural networks is possible but hard a task.

The difficulty of finding optimal structures of neural networks was the main limitation observed in this project. Choice of architecture of a network directly impacted the performance of the model. The number of layers and neurons in each layer also was important for the performance of the model. Moreover, the correct activation function and weight optimisation function had to be selected in order for the classifier to work properly. Finding all these parameters was a very hard task. It was unknown how to approach this problem. Some parameters were correlated ( e.g. learning rate should be increased if a dropout is added), other parameters were connected but it was hard to observe patterns between their values, and some other parameters seemed to be independent from others. 

\section{Summary}

To conclude, transfer learning achieved the highest classification score. However, this model was not built from the ground up. The best model that was built was the convolutional neural network which achieved 69.34 \% accuracy.  It was also stated that deep neural network models are very hard to develop and take much longer time to train when compared to other classification methods. That is a trade-off between using a deep learning model and other machine learning classifier. In the next chapter, the conclusions of the work are deducted.
