
\chapter{Results}
\lhead{\emph{Results}}
In this chapter, the results of each classification method are compared. Then, advantages and disadvantage of these methods are discussed.

\section{Comparison of Accuracy Scores}

Results of the best classification model of each method tried are shown in  \autoref{table:all}. From this table, it can be seen that transfer learning is the best classification method with 98.20 percent accuracy. The convolutional neural network is in the second place with 69.34 percent accuracy. It has to be considered that with transfer learning a classification model was not built from the ground up. The difference in the  accuracy score between other classification methods is not that marginal. The lowest accuracy score of 51.53 percent was achieved by the Linear classifier while the support vector machine model achieved accuracy score of 60.73 percent.  From these results it can be concluded that the CNN achieved a much higher accuracy score than other classifiers that were built from the ground up.

\begin{table}[h]
\begin{center}
\begin{tabular}{ |c|c| } 
 \hline
Method &  Classification Accuracy \\   \hline
KNN    &  56.32\% \\
SVM with PCA   & 60.73\% \\
Linear   &    51.53\%  \\
3 Layer    &   54.02\% \\
5 Layer    &  58.81\% \\
CNN & 69.34\% \\
Transfer  &  98.20\% \\

 \hline
\end{tabular}
\caption{Best Classifiers of Each Method}
\label{table:all}
\end{center}
\end{table}


\section{Comparison  of time complexity of the classifiers}
There are three kinds of time complexities that can be considered for evaluating machine learning classifiers. The first  is a build-time complexity - how long does it take to create a classification model. The second  is a training complexity - the amount that it takes to train a classification model. The final  is a runtime complexity - how long does the model take to classify an item. 

The K-nearest neighbours classifier was simple to build. It only required a hyper-parameter optimisation which did not take long. The training of the algorithm was also fast. In order  to train this algorithm, the dataset was simply saved to the memory. However, the runtime performance of this classifier was not that great. During the runtime, this algorithm has to calculate  distances between an unseen data sample  and every other element in the dataset. Later, these distances are compared to find the K closest distances. Because these are  computationally expensive tasks, it takes a long time for K-NN classifier to make a prediction.

It was a much more difficult task to get Support Vector Machine Algorithm to work. Despite the fact that the algorithm is designed to work on a high dimensional data,  the food image dataset was too complex for it. The only way to use this algorithm was by firstly applying the PCA to the dataset. Therefore, building this model required a significant amount of experimentation and took a much longer time, before actual results were seen. It took a very long time to train the support vector machine. Prediction time was faster than the prediction time of the K-NN classifier but was not instantaneous.

The Linear classifier is a very simple classification model. Therefore, it did not take much time to create this model. Training of this algorithm was fast because the only things calculated were a matrix product of training images and weights, and updated weights, calculated by an optimisation algorithm. Prediction time was almost immediate.

Deep neural network required much more experimentation, to make it work. Training of the algorithm took a long time, but the prediction was almost as fast as the linear classification model.

Convolutional neural network required, even more, engineering work. Some CNN models architectures that were tried had a very low classification accuracy. Various models had to be tested, in order to finally find the architecture that was working.  Training of the algorithm took the longest amount of time from all of the algorithms implemented. However, the classifier was able to predict the classes of new pictures very quickly.

The transfer learning technique was simple because the model with a python script was provided by Google. In a case where one has to write a script that retrains the final layer,  it can be a very hard task. The calculation of the bottlenecks took a long time. Therefore a training complexity of this algorithm was high.  However, for classifying the images, this algorithm took the same time as other neural networks.

To conclude, the algorithm that took the least time to built was KNN. The most time was spent deep neural network models because finding an appropriate network architecture was a very hard task. The classification algorithm that took the least time to train was KNN. For fastest prediction times, the linear classifier was the best option.

\section{Possible improvements and limitations of classification models}

The KNN classifier, the SVM,  and the linear classifier model can not be considerably improved.  Hyperparameters of these algorithms were optimised by using a grid or random search optimisation methods. Therefore, the only possibility for improving these models is training these models on a more powerful machine and  using  a larger sizes of  images.

In contrast,  the accuracy of the deep neural network and the convolutional neural network could still be considerably improved. It can be done by adding more layers or modifying the structure of the networks. However, it can not be mathematically calculate what structures of neural networks work the best. The number of ways that these networks can be configured is infinite. Therefore, finding optimal structures of neural networks is possible but hard task.

That is the main limitation of neural networks that was observed in this project. Choice of  architecture of a network directly impacted the performance of the model. Number of layers and the number of neurons in each layer also were important for performance of the model. Moreover, correct activation function and weight optimisation function had to be selected too, in order for classifier to work properly. Finding all these parameters was very hard task. It was unknown how to approach this problem. Some parameters were correlated ( e.g. learning rate should be increased if a dropout is added), other parameters were connected but it was hard to observe patterns between their values, and values of some other parameters seemed to be independed from others. 

\section{Summary}

To conclude transfer learning achieved the highest classification score. However, this model was not built from the ground up. The best model that was built was the convolutional neural network which achieved 69.34 \% accuracy.  It was also discussed that deep neural network models are very hard to build and take much longer time to train when compared to other classification methods. That is a trade-off between using a deep learning model and other machine learning classifier .In the next chapter, the conclusions of the work are going to be deducted.
